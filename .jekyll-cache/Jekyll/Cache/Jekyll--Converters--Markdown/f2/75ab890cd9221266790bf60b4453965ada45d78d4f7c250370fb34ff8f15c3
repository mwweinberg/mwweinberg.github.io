I"Y<p>A team just announced the release of the <a href="https://github.com/r-three/common-pile/tree/main">Common Pile</a>, a large dataset for training large language models (LLMs).  Unlike other datasets, Common Pile is built exclusively on “openly licensed text.”  On one hand, this is an interesting effort to build a new type of training dataset that illustrates how even the “easy” parts of this process are actually hard. On the other hand, I worry that some people read “openly licensed training dataset” as the equivalent of (or very close to) “LLM free of copyright issues.”</p>
:ET